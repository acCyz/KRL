import pandas as pd
import os
import streamlit as st
from PIL import Image
import numpy as np
import torch
import run

from onlysampling import NegSampler


def get_dir_list(path='./models/'):
    dir_list = os.listdir(path)
    return dir_list


def highlight_best(s):  # è¡¨æ ¼é«˜äº®æœ€ä½³æ•°æ®
    """
    highlight the maximum in a Series yellow.
    highlight_max(subset=['HITS@1','HITS@3','HITS@10','MRR'],
                                               color='red',
                                               axis=0)
    """
    if s.name == 'MR':
        is_list = s == s.min()
    else:
        is_list = s == s.max()
    return ['background-color: yellow' if v else '' for v in is_list]


def highlight_min(s):
    """
    highlight the maximum in a Series yellow.
    highlight_max(subset=['HITS@1','HITS@3','HITS@10','MRR'],
                                               color='red',
                                               axis=0)
    """
    is_min = s == s.min()
    return ['background-color: yellow' if v else '' for v in is_min]


st.set_page_config(
    page_title="easy KRL",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://www.extremelycoolapp.com/help',
        'Report a bug': "https://www.extremelycoolapp.com/bug",
        'About': "# cyz. This is an *extremely* cool app!"
    }
)


info = st.empty()
info1, info2 = info.columns([8, 1])
image = Image.open('./codes/å¤´éƒ¨èƒŒæ™¯1.jpeg')
TopHeader = st.container()
with TopHeader:
    st.title('Hi,æ¬¢è¿ä½¿ç”¨Easy KRL ğŸ‰ğŸ¥‡')
    st.title('ä¸€é”®å›¾è°±åµŒå…¥å·¥å…·')
    st.image(image, use_column_width=True)

tips = st.empty()
with tips.container():
    st.write("ğŸ“¢åœ¨è¿™é‡Œä½ å¯ä»¥è½»æ¾çš„è°ƒé…è®­ç»ƒçŸ¥è¯†è¡¨ç¤ºå‘é‡ç©ºé—´,æˆ‘ä»¬å‡†å¤‡äº†ä»¥ä¸‹åŠŸèƒ½:")
    st.markdown('''
                    - ä¸Šä¼ æˆ–é€‰æ‹©å·²æœ‰æ•°æ®é›†è¿›è¡Œè®­ç»ƒ.
                    - æˆ‘ä»¬å®ç°äº†TransEä½œä¸ºä¸»è¦è®­ç»ƒæ¨¡å‹.
                    - åœ¨TransEä¸­é€‚é…äº†Uniform Sampling, SANS å’Œ NSCaching ä¸‰ç§Sampling method.
                    - æ”¯æŒè®­ç»ƒå®æ—¶åé¦ˆå’Œè´Ÿé‡‡æ ·åˆ†æä»¥åŠæ¨¡å‹æŒ‡æ ‡å¯¹æ¯”.
                ''')

    col0, col1, col2 = st.columns([1, 1, 1])

    col0.write('ğŸš€__å¦‚ä½•å¼€å§‹__')
    col0.markdown('''
                        - &#x1F448 èœå•æ é€‰æ‹©æ•°æ®é›†
                        - æˆ‘ä»¬å·²ç»ä¸ºä½ å‡†å¤‡äº†
                        - ğŸ“—FB15k-237.
                        - ğŸ“—wn18
                        - ğŸ“—wn18rr
                        - ğŸ“—yago3-10
                        - æˆ–è€…ä¸Šä¼ ä½ è‡ªå·±çš„æ•°æ®é›†.
                    ''')
    col0.image("https://static.streamlit.io/examples/cat.jpg")

    col1.write('ğŸš€__è´Ÿé‡‡æ ·åˆ†æ__')
    col1.markdown('''
                            - æˆ‘ä»¬å®ç°äº†ä»¥ä¸‹è´Ÿé‡‡æ ·æ–¹æ³•
                            - ğŸˆUniform
                            - ğŸˆSANS 
                            - ğŸˆNSCaching
                            - ğŸˆ....
                            - ä½ å¯ä»¥åŸºäºä¸åŒæ•°æ®é›†è¿›è¡Œè´Ÿé‡‡æ ·.
                        ''')
    col1.image("https://static.streamlit.io/examples/cat.jpg")

    col2.write("ğŸš€__æ¨¡å‹è®­ç»ƒå’Œå¯¹æ¯”__")
    col2.markdown('''
                         - ç›®å‰è®­ç»ƒæ¨¡å‹æ”¯æŒTransE
                         - ä½ å¯ä»¥é…ç½®ä¸åŒå‚æ•°è®­ç»ƒï¼Œå¹¶æ”¯æŒä»¥ä¸‹åŠŸèƒ½
                         - ğŸ„å®æ—¶è®­ç»ƒåé¦ˆ
                         - ğŸƒå¤šæ¨¡å‹æŒ‡æ ‡å¯¹æ¯”
                         - ğŸš´å®ä½“é“¾æ¥é¢„æµ‹
                         - ğŸƒæ›´å¤šåŠŸèƒ½æœ‰å¾…å¼€å‘
                            ''')
    col2.image("https://static.streamlit.io/examples/dog.jpg")

# st.image(Image.open('D:\\KRL\\codes\\æ¡†æ¶.png'), use_column_width=True)

# st.sidebar.image(image, caption='Sunrise by the mountains', use_column_width=True)
# st.sidebar.title("èœå•")

dataset_path = './data/'
models_path = './models/'
dataset_list = get_dir_list(dataset_path)
st.sidebar.header("é€‰æ‹©æ•°æ®é›†å’Œä½ è¦æ‰§è¡Œçš„ä»»åŠ¡")
dataset_list.insert(0, 'ä¸Šä¼ æ•°æ®é›†')
dataset = st.sidebar.selectbox("é€‰æ‹©æ•°æ®é›†", dataset_list)

if dataset == 'ä¸Šä¼ æ•°æ®é›†':
    with st.sidebar.expander('ä¸€æ¬¡æ€§ä¸Šä¼ ï¼ˆæ‰¹é‡ä¸Šä¼ æ‰€æœ‰æ–‡ä»¶ï¼‰', expanded=True):
        file_list = st.file_uploader('è¯·æŒ‰e2id/r2id/è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†é¡ºåºé€‰æ‹©æ–‡ä»¶', accept_multiple_files=True)
    with st.sidebar.expander('åˆ†æ­¥ä¸Šä¼ ï¼ˆå•ç‹¬ä¸Šä¼ æ¯ä¸€ä¸ªæ–‡ä»¶ï¼‰'):
        e2id = st.file_uploader('ä¸Šä¼ e2idæ–‡ä»¶(å¿…é¡»)')
        r2id = st.file_uploader('ä¸Šä¼ r2idæ–‡ä»¶(å¿…é¡»)')
        train_data = st.file_uploader('ä¸Šä¼ è®­ç»ƒé›†(å¿…é¡»)')
        valid_data = st.file_uploader('ä¸Šä¼ éªŒè¯é›†')
        test_data = st.file_uploader('ä¸Šä¼ æµ‹è¯•é›†')
    st.sidebar.write("")
    dataset_name = st.sidebar.text_input("è¾“å…¥ä¸Šä¼ çš„æ•°æ®é›†åç§°")
    upload = st.sidebar.button("ç¡®è®¤ä¸Šä¼ ")
    if upload:
        if len(file_list) < 3 or (e2id and r2id and train_data):
            info1.error("æœªå®Œæ•´ä¸Šä¼ å¿…é¡»çš„e2id/r2id/è®­ç»ƒé›†ï¼Œè¯·é‡æ–°ä¸Šä¼ ï¼")
        elif os.path.exists(os.path.join(dataset_path, dataset_name)):
            info1.warning(f'åä¸º{dataset_name}çš„æ•°æ®é›†å·²å­˜åœ¨ï¼')
        else:
            os.makedirs(os.path.join(dataset_path, dataset_name))
            info1.success("ä¸Šä¼ æˆåŠŸï¼Œæ•°æ®é›†åˆ—è¡¨å·²æ›´æ–°ï¼Œè¯·é‡æ–°é€‰æ‹©ï¼")
        info2.button('ç¡®è®¤')
    st.stop()

entity2id, relation2id, train_triples, valid_triples, test_triples, all_symbol_triples \
    = run.read_data(f'data/{dataset}')

h_r, hr_t = run.create_cache(all_symbol_triples)

menu = st.sidebar.selectbox("é€‰æ‹©æ‰§è¡Œä»»åŠ¡", ['None', 'ä¸Šä¼ æ•°æ®é›†', 'è´Ÿé‡‡æ ·åˆ†æ', 'æ¨¡å‹è®­ç»ƒ', 'æ¨¡å‹æµ‹è¯•', 'å®ä½“é¢„æµ‹'])
if menu == 'None':
    st.stop()
elif menu == 'ä¸Šä¼ æ•°æ®é›†':
    with st.sidebar.expander('ä¸€æ¬¡æ€§ä¸Šä¼ ï¼ˆæ‰¹é‡ä¸Šä¼ æ‰€æœ‰æ–‡ä»¶ï¼‰', expanded=True):
        file_list = st.file_uploader('è¯·æŒ‰e2id/r2id/è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†é¡ºåºé€‰æ‹©æ–‡ä»¶', accept_multiple_files=True)
    with st.sidebar.expander('åˆ†æ­¥ä¸Šä¼ ï¼ˆå•ç‹¬ä¸Šä¼ æ¯ä¸€ä¸ªæ–‡ä»¶ï¼‰'):
        e2id = st.file_uploader('ä¸Šä¼ e2idæ–‡ä»¶(å¿…é¡»)')
        r2id = st.file_uploader('ä¸Šä¼ r2idæ–‡ä»¶(å¿…é¡»)')
        train_data = st.file_uploader('ä¸Šä¼ è®­ç»ƒé›†(å¿…é¡»)')
        valid_data = st.file_uploader('ä¸Šä¼ éªŒè¯é›†')
        test_data = st.file_uploader('ä¸Šä¼ æµ‹è¯•é›†')
    st.sidebar.write("")
    dataset_name = st.sidebar.text_input("è¾“å…¥ä¸Šä¼ çš„æ•°æ®é›†åç§°")
    upload = st.sidebar.button("ç¡®è®¤ä¸Šä¼ ")
    if upload:
        if len(file_list) < 3 or (e2id and r2id and train_data):
            info1.error("æœªå®Œæ•´ä¸Šä¼ å¿…é¡»çš„e2id/r2id/è®­ç»ƒé›†ï¼Œè¯·é‡æ–°ä¸Šä¼ ï¼")
        elif os.path.exists(os.path.join(dataset_path, dataset_name)):
            info1.warning(f'åä¸º{dataset_name}çš„æ•°æ®é›†å·²å­˜åœ¨ï¼')
        else:
            os.makedirs(os.path.join(dataset_path, 'data/', dataset_name))
            info1.success("ä¸Šä¼ æˆåŠŸï¼Œæ•°æ®é›†åˆ—è¡¨å·²æ›´æ–°ï¼Œè¯·é‡æ–°é€‰æ‹©ï¼")
        info2.button('ç¡®è®¤')
    st.stop()
elif menu == 'è´Ÿé‡‡æ ·åˆ†æ':
    # form = st.sidebar.form("submit_form")
    sampling_mode = st.sidebar.multiselect("é€‰æ‹©è´Ÿé‡‡æ ·æ–¹æ³•", ['Uniform', 'uniform-SANS', 'rw-SANS', 'NSCaching'])
    nsample = st.sidebar.number_input("é€‰æ‹©è´Ÿæ ·æœ¬ç”Ÿæˆæ•°é‡", 1, 30, value=10)
    useCache = st.sidebar.checkbox("ä½¿ç”¨ç¼“å­˜åº“", value=True)
    start_sampling = st.sidebar.button("Go!")
    # sampling_container = st.beta_container()
    with st.expander(f'ğŸ‘‰ç‚¹å‡»å±•å¼€ï¼Œé…ç½®{[i for i in sampling_mode]}è´Ÿé‡‡æ ·å…·ä½“å‚æ•°'):
        if 'Uniform' in sampling_mode:
            param = st.empty()
        if 'uniform-SANS' in sampling_mode:
            st.write("uniform-SANS")
            col1, col2 = st.columns(2)
            with col1:
                usans_k = st.number_input("é€‰æ‹©uniform-SANSé‚»åŸŸk", 1, 10, value=3)
            with col2:
                usans_w = st.number_input("é€‰æ‹©éšæœºæ¼«æ­¥æ¬¡æ•°w", 0, 0)
        if 'rw-SANS' in sampling_mode:
            st.write("rw-SANS")
            col1, col2 = st.columns(2)
            with col1:
                rsans_k = st.number_input("é€‰æ‹©rw-SANSé‚»åŸŸk", 1, 10, value=3)
            with col2:
                rsans_w = st.number_input("é€‰æ‹©éšæœºæ¼«æ­¥æ¬¡æ•°w", 0, 5000, value=2000)
        if 'NSCaching' in sampling_mode:
            st.write("NSCaching")
            col1, col2 = st.columns(2)
            with col1:
                nsc_n1 = st.number_input("é€‰æ‹©ç¼“å­˜å¤§å°N1", 10, max(100, nsample), value=30)
            with col2:
                nsc_n2 = st.number_input("é€‰æ‹©æ›´æ–°å­é›†å¤§å°N2", 10, 100, value=30)

    st.write('')
    st.write('ğŸ‘‡ä»å›¾è°±ä¸­é€‰æ‹©è¦ç”Ÿæˆè´Ÿæ ·æœ¬çš„ä¸‰å…ƒç»„ï¼š')

    h, r, t, mode = st.columns(4)
    h_list = list(entity2id.keys())
    # h_list = tran2mean(h_list, english)
    h_selected = h.selectbox("é€‰æ‹©å¤´å®ä½“", h_list)

    r_list = h_r[h_selected]
    r_selected = r.selectbox("é€‰æ‹©å…³ç³»", r_list)

    t_list = hr_t[(h_selected, r_selected)]
    # t_list = tran2mean(t_list, english)
    t_selected = t.selectbox("é€‰æ‹©å°¾å®ä½“", [t_list])

    mode = mode.selectbox("é€‰æ‹©æ‰“ç¢çš„ä½ç½®", ['å¤´å®ä½“', 'å°¾å®ä½“', 'å…³ç³»'])

    testinfo = st.empty()

    sampler = NegSampler(train_triples, len(entity2id), len(relation2id),
                         nsample, 'head-batch', f'data/{dataset}')
    # æ ¡éªŒå‚æ•°åˆæ³•æ€§
    if start_sampling:
        negative_sample_list = {}
        testinfo.info("è´Ÿé‡‡æ ·ä¸­........")
        for nsm in sampling_mode:
            arg = []
            nsm_ = ''
            if nsm == 'uniform-SANS':
                k = usans_k
                rw = usans_w
                nsm_ = 'sans'
                arg.append(k)
                arg.append(rw)
            if nsm == 'rw-SANS':
                k = rsans_k
                rw = rsans_w
                nsm_ = 'sans'
                arg.append(k)
                arg.append(rw)
            if nsm == 'Uniform':
                nsm_ = 'uniform'
            if nsm == 'NSCaching':
                nsm_ = 'nscaching'
                arg.append(nsc_n1)
                arg.append(nsc_n2)
                if not os.path.exists(f'nscache/{dataset}_head_{nsc_n1}n1_{nsc_n2}n2.npy'):
                    st.error("ç”±äºNSCAchingä¸ºåŠ¨æ€è´Ÿé‡‡æ ·ï¼Œå› æ­¤ç›®å‰åªæ”¯æŒä»ä»¥å¾€è®­ç»ƒçš„æ¨¡å‹ç¼“å­˜ä¸­å±•ç¤ºï¼Œæ‚¨å½“å‰é€‰æ‹©çš„NSCachingé‡‡æ ·å‚æ•°æ‰¾ä¸åˆ°ç¼“å­˜ç»“æœï¼Œè¯·æ›´æ”¹å‚æ•°æˆ–å…ˆä½¿ç”¨è¯¥å‚æ•°è¿›è¡Œæ¨¡å‹è®­ç»ƒ")
                    continue
            negative_sample = sampler.sample(
                (entity2id[h_selected], relation2id[r_selected], entity2id[t_selected]),
                nsm_, arg)
            negative_sample = [list(entity2id.keys())[list(entity2id.values()).index(v)] for v in negative_sample]
            negative_sample_list[nsm] = negative_sample

        # st.write(negative_sample_list)
        st.write('ğŸ‘Œå„è´Ÿé‡‡æ ·æ–¹æ³•ç”Ÿæˆè´Ÿæ ·æœ¬å¦‚ä¸‹ï¼š')

        sample_result_table = st.table(negative_sample_list)
        testinfo.success("è´Ÿé‡‡æ ·å®Œæˆï¼")
        # è¿è¡Œç»“æœ
        st.balloons()
        x = b'aaaaaaaaa'
        st.download_button(
            label='ä¸‹è½½',
            data = x,
            file_name= "a.png"
        )

    st.stop()

elif menu == 'æ¨¡å‹è®­ç»ƒ':
    form = st.sidebar.form("submit_form")
    train_args = [['--do_train', '--data_path', f'data/{dataset}']]
    with form:
        train_model = st.selectbox("é€‰æ‹©è®­ç»ƒæ¨¡å‹", ['TransE'])
        train_args.append(['--model', f'{train_model}'])
        with form.expander(f'ç‚¹å‡»å±•å¼€é…ç½®{train_model}è®­ç»ƒå‚æ•°'):
            epoch = st.number_input("è¾“å…¥epoch", 1000, step=1000, format='%d', value=int(6000))
            lr = st.number_input("è¾“å…¥learn rate", 0.00001, format='%f', value=0.001)
            gamma = st.number_input("è¾“å…¥ä¼˜åŒ–è·ç¦»", 1.0, 12.0, format='%f', step=1.0)
            batch_size = st.number_input("è¾“å…¥batch_size", 100, 2000, format='%d', value=500)
            train_args.append(['--max_steps', f'{epoch}',
                               '-lr', f'{lr}',
                               '-g', f'{gamma}',
                               '-b', f'{batch_size}'])
            if train_model == 'TransE':
                dim = st.number_input("è¾“å…¥åµŒå…¥ç»´åº¦", 100, 1000, format='%d')
                train_args.append(['-d', f'{dim}'])
        form.write("")
        sampling_mode = st.selectbox("é€‰æ‹©è´Ÿé‡‡æ ·æ–¹æ³•", ['uniform', 'sans', 'nscaching'])
        train_args.append(['-nsm', f'{sampling_mode}'])
        with form.expander(f'ç‚¹å‡»å±•å¼€é…ç½®{sampling_mode}è´Ÿé‡‡æ ·å‚æ•°'):
            nsample = st.number_input("é€‰æ‹©è´Ÿæ ·æœ¬ç”Ÿæˆæ•°é‡", 10, 1024, format='%d')
            train_args.append(['-n', f'{nsample}'])
            if 'uniform' == sampling_mode:
                param = st.empty()
            if 'sans' == sampling_mode:
                sans_k = st.number_input("é€‰æ‹©SANSé‚»åŸŸk", 1, 10, format='%d')
                sans_rw = st.number_input("é€‰æ‹©SANSéšæœºæ¸¸èµ°nrw", 0, 5000, format='%d')
                train_args.append(['-khop', f'{sans_k}',
                                   '-nrw', f'{sans_rw}'])
            if 'nscaching' == sampling_mode:
                nsc_n1 = st.number_input("é€‰æ‹©ç¼“å­˜å¤§å°N1", 10, max(100, nsample), format='%d', value=30)
                nsc_n2 = st.number_input("é€‰æ‹©æ›´æ–°å­é›†å¤§å°N2", 10, 100, format='%d', value=30)
                train_args.append(['-ncs', f'{nsc_n1}',
                                   '-nss', f'{nsc_n2}'])

        train_args.append(['-save', f'models/{train_model}_{dataset}_{sampling_mode}_test'])

        st.write("")
        valid = st.checkbox("è®­ç»ƒåŒæ—¶åœ¨éªŒè¯é›†ä¸ŠéªŒè¯", value=True)
        if valid:
            valid_steps = st.number_input("é€‰æ‹©éªŒè¯å‘¨æœŸ", 100, 1000, format='%d', value=200)
            train_args.append(['--do_valid', '--valid_steps', f'{valid_steps}'])

        test = st.checkbox("è®­ç»ƒç»“æŸåæµ‹è¯•", value=True)
        if test:
            test_size = st.number_input("é€‰æ‹©æµ‹è¯•æ—¶çš„batchå¤§å°", 4, 100, format='%d', value=16)
            train_args.append(['--do_test', '--test_batch_size', f'{test_size}'])

        col1, col2 = st.columns([1, 5])
        start_train = col1.form_submit_button("Go!")
        useGPU = col2.checkbox("use GPU")

    # æ ¡éªŒå‚æ•°åˆæ³•æ€§
    train_args = [j for i in train_args for j in i]
    # st.write(train_args)
    if start_train:

        tips.empty()

        # è¿è¡Œç»“æœ
        # st.write(train_args)
        # print(train_args) #parse_args(train_args)
        last_rows = [0.0]  # np.random.randn(1, 1)
        # st.write(last_rows)
        if not valid:
            st.write('è®­ç»ƒè¿‡ç¨‹å®æ—¶lossæ›²çº¿ï¼š[X-epoch / Y-loss]')
            loss_chart = st.line_chart()
            valid_chart = None
        else:
            chart1, chart2 = st.columns([3, 2])
            with chart1:
                st.write('è®­ç»ƒè¿‡ç¨‹å®æ—¶lossæ›²çº¿ï¼š[X-epoch / Y-loss]')
                loss_chart = st.line_chart()
            with chart2:
                st.write('è®­ç»ƒè¿‡ç¨‹åœ¨éªŒè¯é›†ä¸ŠæŒ‡æ ‡å˜åŒ–')
                valid_chart = st.line_chart()
            dic = {0: {'HITS@1': 0.0,
                       'HITS@3': 0.0,
                       'HITS@10': 0.0,
                       'MRR': 0.0}}
            valid_chart.add_rows(pd.DataFrame.from_dict(dic, orient='index'))

        progress_bar = st.progress(0)
        status_text = st.empty()
        test_table = st.empty()

        with st.spinner(f'æ¨¡å‹è®­ç»ƒä¸­ï¼šæ•°æ®é›†[{dataset}]  |  ç¿»è¯‘æ¨¡å‹[{train_model}]  |  è´Ÿé‡‡æ ·ç®—æ³•[{sampling_mode}]'):
            run.run_model(run.parse_args(train_args), 'train',
                      [last_rows, loss_chart, progress_bar, status_text, valid_chart, test_table])
        status_text.write("è®­ç»ƒè¿›åº¦:100%")
        st.success(f'è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³: models/{train_model}_{dataset}_{sampling_mode}_test')
        st.balloons()
    st.stop()


elif menu == 'æ¨¡å‹æµ‹è¯•':
    test_args = [['--do_test']]

    isall_model = st.sidebar.checkbox("æ˜¾ç¤ºå…¨éƒ¨æ¨¡å‹ï¼ˆä¸é™æ•°æ®é›†ï¼‰")
    model_list = get_dir_list(models_path)
    if not isall_model:
        model_list = [x for x in model_list if dataset in x.split('_')]

    form = st.sidebar.form("submit_form")
    with form:
        # æ£€æŸ¥æµ‹è¯•é›†å®Œæ•´æ€§
        test_models = st.multiselect("é€‰æ‹©æµ‹è¯•æ¨¡å‹", model_list)

        test_batch_size = st.number_input("è¾“å…¥æµ‹è¯•batch_size", 16, 1000, format='%d')
        test_args.append(['--test_batch_size', f'{test_batch_size}'])

        start_test = st.form_submit_button("Go!")

    test_info = st.empty()
    st.write("ğŸ‘‡overall æŒ‡æ ‡å¯¹æ¯”ç»“æœå¦‚ä¸‹ï¼Œæ¯é¡¹æŒ‡æ ‡çš„æœ€ä¼˜å€¼å·²ç»é«˜äº®ï¼š")

    test_result_table = st.dataframe()
    test_result_chart = st.bar_chart()
    # bar_chartç»„ä»¶ä¼šæŠ¥UserWarning: I don't know how to infer vegalite type from 'empty'.  Defaulting to nominal.
    # å› ä¸ºæœªå¼€å§‹æ‰§è¡Œçš„æ—¶å€™è¿˜æ²¡æœ‰å€¼ï¼Œè¡¨æ˜¯ç©ºçš„ï¼Œå¯ä»¥å¿½ç•¥è¿™ä¸ªè­¦å‘Š
    index = ['MRR', 'MR', 'HITS@1', 'HITS@3', 'HITS@10']

    HITS1_index = st.expander('ğŸ‘‰HITS@1å¯¹æ¯”')
    HITS1_index1, HITS1_index2 = HITS1_index.columns([1, 1])
    HITS1_index_chart = HITS1_index1.bar_chart()
    HITS1_index_table = HITS1_index2.dataframe()

    HTIS3_index = st.expander('ğŸ‘‰HITS@3å¯¹æ¯”')
    HITS3_index1, HITS3_index2 = HTIS3_index.columns([1, 1])
    HITS3_index_chart = HITS3_index1.bar_chart()
    HITS3_index_table = HITS3_index2.dataframe()

    HITS10_index = st.expander('ğŸ‘‰HITS@10å¯¹æ¯”')
    HITS10_index1, HITS10_index2 = HITS10_index.columns([1, 1])
    HITS10_index_chart = HITS10_index1.bar_chart()
    HITS10_index_table = HITS10_index2.dataframe()

    MRR_index = st.expander('ğŸ‘‰MRRå¯¹æ¯”')
    MRR_index1, MRR_index2 = MRR_index.columns([1, 1])
    MRR_index_chart = MRR_index1.bar_chart()
    MRR_index_table = MRR_index2.dataframe()

    MR_index = st.expander('ğŸ‘‰MRå¯¹æ¯”')
    MR_index1, MR_index2 = MR_index.columns([1, 1])
    MR_index_chart = MR_index1.bar_chart()
    MR_index_table = MR_index2.dataframe()

    test_args = [j for i in test_args for j in i]
    # æ ¡éªŒæ¨¡å‹å®Œæ•´æ€§
    if start_test:
        tips.empty()

        test_args.append('-init')
        table_metrics_all = pd.DataFrame()
        for model in test_models:
            init_path = os.path.join('models/', model)
            if os.path.exists(os.path.join(init_path, 'test.log')):
                test_info.info(f'æ¨¡å‹{model}ä½¿ç”¨ç¼“å­˜æµ‹è¯•ç»“æœ......')
                test_result = run.read_cached_test(init_path)

                table_metrics = {f'{model}': test_result}
                table_metrics = pd.DataFrame.from_dict(table_metrics, orient='index')
                table_metrics_all = pd.concat([table_metrics_all, table_metrics])
                # table_metrics.style.highlight_max(subset=['HITS@10'], color='red', axis=0)

                test_result_temp = test_result.copy()
                test_result_temp.pop('MR')
                without_MR = {f'{model}': test_result_temp}
                chart_metrics = pd.DataFrame.from_dict(without_MR, orient='columns')  # columns

                test_result_table.add_rows(table_metrics)
                test_result_chart.add_rows(chart_metrics)
                # dict([(key, base[key]) for key in subkey])
                HITS1_index_chart.add_rows(pd.DataFrame.from_dict({model: {'hits@1': test_result['HITS@1']}},
                                                                  orient='index'))
                HITS1_index_table.add_rows(pd.DataFrame.from_dict({model: {'HITS@1': test_result['HITS@1']}},
                                                                  orient='index'))
                HITS3_index_chart.add_rows(pd.DataFrame.from_dict({model: {'hits@3': test_result['HITS@3']}},
                                                                  orient='index'))
                HITS3_index_table.add_rows(pd.DataFrame.from_dict({model: {'HITS@3': test_result['HITS@3']}},
                                                                  orient='index'))
                HITS10_index_chart.add_rows(pd.DataFrame.from_dict({model: {'hits@10': test_result['HITS@10']}},
                                                                   orient='index'))
                HITS10_index_table.add_rows(pd.DataFrame.from_dict({model: {'HITS@10': test_result['HITS@10']}},
                                                                   orient='index'))
                MRR_index_chart.add_rows(pd.DataFrame.from_dict({model: {'MRR': test_result['MRR']}},
                                                                orient='index'))
                MRR_index_table.add_rows(pd.DataFrame.from_dict({model: {'MRR': test_result['MRR']}},
                                                                orient='index'))
                MR_index_chart.add_rows(pd.DataFrame.from_dict({model: {'MR': test_result['MR']}},
                                                               orient='index'))
                MR_index_table.add_rows(pd.DataFrame.from_dict({model: {'MR': test_result['MR']}},
                                                               orient='index'))

            else:
                cur_test_args = test_args + [init_path]
                # st.write(cur_test_args)
                test_info.info(f'æ¨¡å‹{model}åœ¨æ•°æ®é›†{model.split("_")[1]}ä¸Šæµ‹è¯•ä¸­......')
                run.run_model(run.parse_args(cur_test_args), 'test', [0, 0, 0, 0, 0, test_result_table])

        # table_metrics_all = merge_dict(table_metrics_all)
        # table_metrics_all = pd.DataFrame.from_dict(table_metrics_all, orient='index')
        # st.write(table_metrics_all)
        test_result_table.dataframe(table_metrics_all.style.apply(highlight_best))

        # è¿è¡Œç»“æœ
        test_info.info("æ‰€æœ‰æ¨¡å‹æµ‹è¯•å®Œæˆ!")
        st.balloons()
    st.stop()

elif menu == 'å®ä½“é¢„æµ‹':
    model_list = get_dir_list(models_path)
    model_list = [x for x in model_list if dataset in x.split('_')]
    test_model = st.sidebar.multiselect("é€‰æ‹©æ¨¡å‹éƒ¨ç½²", model_list)
    predict = st.sidebar.selectbox("é€‰æ‹©è¡¥å…¨æ–¹å¼", ['å…³ç³»é¢„æµ‹', 'å¤´å®ä½“é¢„æµ‹', 'å°¾å®ä½“é¢„æµ‹'])
    # useless_predict = st.sidebar.checkbox("")
    start = st.sidebar.button("GO!")
    e_list = list(entity2id.keys())
    r_list = list(relation2id.keys())
    col1, col2 = st.columns(2)
    if predict == 'å…³ç³»é¢„æµ‹':
        a = col1.selectbox("å¤´å®ä½“", e_list)
        b = col2.selectbox("å°¾å®ä½“", e_list)
    elif predict == 'å¤´å®ä½“é¢„æµ‹':
        a = col1.selectbox("å…³ç³»", r_list)
        b = col2.selectbox("å°¾å®ä½“", e_list)
    else:
        a = col1.selectbox("å¤´å®ä½“", e_list)
        r_list = h_r[a]
        b = col2.selectbox("å…³ç³»", r_list)

    embed_list = []
    # r_embed_list = []
    for model in test_model:
        e_embed = np.load(os.path.join(models_path, model, 'entity_embedding.npy'))
        r_embed = np.load(os.path.join(models_path, 'models/', model, 'relation_embedding.npy'))
        e_embed = torch.from_numpy(e_embed)
        r_embed = torch.from_numpy(r_embed)
        embed_list.append([e_embed, r_embed])
        # r_embed_list.append(r_embed)
    # st.write(e_embed)
    id2entity = {int(v): k for k, v in entity2id.items()}
    id2relation = {int(v): k for k, v in relation2id.items()}

    if start:
        score_list = []
        result_list = []
        for embed in embed_list:
            if predict == 'å…³ç³»é¢„æµ‹':
                score = (embed[0][entity2id[a]] - embed[0][entity2id[b]]) + embed[1]
            elif predict == 'å¤´å®ä½“é¢„æµ‹':
                score = embed[0] + (embed[1][relation2id[a]] - embed[0][entity2id[b]])
            else:
                score = (embed[0][entity2id[a]] + embed[1][relation2id[b]]) - embed[0]
            score = torch.norm(score, p=1, dim=-1)
            topk, index = torch.topk(score, 10, largest=False)
            topk = [x.item() for x in topk]
            if predict == 'å…³ç³»é¢„æµ‹':
                result = [id2relation[x.item()] for x in index]
            else:
                result = [id2entity[x.item()] for x in index]
            score_list.append(topk)
            result_list.append(result)

        i = 0
        over_all = {}
        for model in test_model:
            over_all[model] = result_list[i]
            i = i + 1
        st.write("ğŸ‘‡å„æ¨¡å‹çš„é¢„æµ‹å‰åç»“æœå¦‚ä¸‹ï¼ŒæŒ‰å¾—åˆ†ä»é«˜åˆ°ä½æ’åºï¼š")
        st.dataframe(over_all)

        i = 0
        for model in test_model:
            with st.expander('ğŸ‘‰ç‚¹å‡»å±•å¼€ ' + model + ' è¯¦ç»†é¢„æµ‹å¾—åˆ†æƒ…å†µ'):
                # st.write(score_list[i])
                # st.write(result_list[i])
                data = {'é¢„æµ‹ç»“æœ': result_list[i], 'å¾—åˆ†': score_list[i]}
                st.table(data)

            i = i + 1

        # st.write(score_list, result_list)

        st.stop()
        # st.write(score)
        # st.write(score.size())
        # st.write(score[115],score[116],score[117])
        # st.write(min(score))

# run_model(parse_args())
# streamlit run D:/testcode/KGtest/codes/run.py